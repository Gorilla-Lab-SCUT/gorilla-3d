task: train  # train, test
seed: 123
epochs: 384

dataset:
  type: ScanNetV2Inst
  data_root: data/scannetv2
  full_scale: [128, 512]
  scale: 50   # voxel_size = 1 / scale, scale 50(2cm)
  max_npoint: 250000
  with_elastic: False

dataloader:
  batch_size: 4
  num_workers: 8
  pin_memory: True

solver:
  # train mode
  epochs: 384
  save_freq: 16  # also eval_freq

data:
  mode: 4 # 4=mean
  # test mode
  split: val
  test_epoch: 384
  test_seed: 567
  test_workers: 8 # data loader workers

  TEST_NMS_THRESH: 0.3
  TEST_SCORE_THRESH: 0.09
  TEST_NPOINT_THRESH: 100

  eval: True
  save_semantic: False
  save_pt_offsets: False
  save_instance: False

model:
  type: PointGroup
  input_channel: 3
  blocks: 5
  media: 32 # 16 or 32
  block_reps: 2
  classes: 20

  prepare_epochs: 128
  use_coords: True
  fix_module: []

  score_scale: 50 # the minimal voxel size is 2cm
  score_fullscale: 14
  score_mode: 4 # mean

  ### point grouping cluster parameters
  cluster_cfg:
    radius: 0.04
    radius_shift: 0.03
    mean_active: 50
    shift_mean_active: 300
    npoint_thresh: 50

loss:
  type: PointGroupLoss
  ignore_label: -100
  prepare_epochs: 128
  fg_thresh: 0.75
  bg_thresh: 0.25
  loss_weight: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # semantic_loss, offset_norm_loss, offset_dir_loss, score_loss

# optimizer
optimizer:
  lr: 0.001
  # type: Adam
  type: AdamW
  weight_decay: 0.0001
  # amsgrad: False

# lr_scheduler
lr_scheduler:
  type: PolyLR
  max_iters: 512
  power: 0.9
  constant_ending: 0.0

